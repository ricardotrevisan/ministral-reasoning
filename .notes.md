
# Compiling llama.cpp with CUDA support:

## Update
``` bash
sudo apt update
sudo apt install -y build-essential cmake python3-dev
```

## Repository Clone
``` bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
```


## Build 

Dest folder:
``` bash
rm -rf build
mkdir build
cd build
```

Config:
``` bash
cmake .. -DGGML_CUDA=ON -DGGML_CUDA_SM=86
```

Building fast:
``` bash
cmake --build . --config Release -j
```

Testing:
``` bash
./bin/llama-cli -m ../models/algum_modelo.gguf -p "hello"
```

Installing llama-cpp-python with CUDA support now:
``` bash
CMAKE_ARGS="-DGGML_CUDA=on -DGGML_CUDA_SM=86" \
pip install llama-cpp-python --force-reinstall --no-cache-dir --upgrade
```

---

# Completions:

``` bash
~/repositories/model-eval-ministral-3-8B-reasoning$ ../llama.cpp/build/bin/llama-server   -m Ministral-3-8B-Reasoning-2512-Q5_K_M.gguf   --port 8080   --host 127.0.0.1   --threads 8   --gpu-layers 999
```

Legacy:
``` bash
curl http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
        "prompt": "<s>[INST] Explique 2+2. [/INST]",
        "temperature": 0.7,
        "top_p": 0.9,
        "max_tokens": 200,
        "stop": ["</s>"]
      }'
```

Testing v1/chat/completions:
``` bash
curl http://127.0.0.1:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "Ministral-3-8B-Reasoning-2512-Q5_K_M.gguf",
        "messages": [
          {"role": "user", "content": "Explique 2+2 de forma objetiva e curta."}
        ]
      }'
```

With instructions (given a reasoning model):
``` bash
curl http://127.0.0.1:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
  "model": "Ministral-3-8B-Reasoning-2512-Q5_K_M.gguf",
  "messages": [
    {
      "role": "system",
      "content": "Você é um assistente conciso. Responda de forma curta e direta."
    },
    {
      "role": "user",
      "content": "Explique 2+2 em uma frase."
    }
  ],
  "max_tokens": 32,
  "temperature": 0.2
}'
```
